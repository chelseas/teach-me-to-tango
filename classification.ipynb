{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Network\n",
    "\n",
    "The goal here is to classify whether the error rate is \"low\" \"medium\" or \"high\".\n",
    "If we can successfully do this, then look at whether we can get the sign right. Start off by using the squeezenet model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def get_session():\n",
    "    \"\"\"Create a session that dynamically allocates memory.\"\"\"\n",
    "    # See: https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(flows_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Pre-processing data...\n",
      "Organizing train/val split...\n",
      "(6756,)\n",
      "4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/__main__.py:94: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Import chelsea's data\n",
    "# Check times versus times.csv\n",
    "# [t, x_tango, y_tango, z_tango, x_vicon, y_vicon, z_vicon, x_err_rate, y_err_rate, z_err_rate]\n",
    "# Error rates, xyz\n",
    "print('Loading data...')\n",
    "d_raw = (np.genfromtxt('trasnformed_data_slash_2.csv',delimiter=',')).astype(np.float32)\n",
    "t_raw = d_raw[:,0]\n",
    "t_img = (np.genfromtxt('rates.csv',delimiter=',')[:,0]).astype(np.float32)\n",
    "\n",
    "## Align start of data streams\n",
    "i_start = 0\n",
    "for k in range(t_img.shape[0]):\n",
    "    if(t_img[k]>t_raw[0]):\n",
    "        i_start = k\n",
    "        break\n",
    "if i_start is 0:\n",
    "    print('Warning! Time sequence alignment failed!')\n",
    "\n",
    "# Extract and interpolate data\n",
    "t_img = t_img[i_start:]\n",
    "imu_data = (np.genfromtxt('rates.csv', delimiter=',')[i_start:,1:7]).astype(np.float32)\n",
    "y_data = np.array([np.interp(t_img, t_raw, d_raw[:,7]), np.interp(t_img, t_raw, d_raw[:,8]), np.interp(t_img, t_raw, d_raw[:,9])]).T\n",
    "\n",
    "\n",
    "pos_err = np.zeros(d_raw.shape[0])\n",
    "for k in range(d_raw.shape[0]):\n",
    "    pos_err[k] = np.linalg.norm(d_raw[k,1:4]-d_raw[k,4:7])\n",
    "pos_err_rate = np.diff(pos_err);\n",
    "\n",
    "# Should really automate this... or just re-save file.\n",
    "flows_data = np.load('../gbucket/center_cropped_300x300.npy').astype(np.float32)\n",
    "#flows_data = []\n",
    "#flows_data.append(np.load('flows_lowres_1_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_2_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_3_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_4_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_5_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_6_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_7_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_8_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_9_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_10_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_11_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_12_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_13_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_14_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_15_16.npy'))\n",
    "#flows_data.append(np.load('flows_lowres_16_16.npy'))\n",
    "#flows_data = np.concatenate(flows_data,axis=0)\n",
    "flows_data = flows_data[i_start:,:,:]\n",
    "\n",
    "\n",
    "print('Pre-processing data...')\n",
    "# Shorten datastream:\n",
    "i_start = 600 # remove initial weird-ness\n",
    "i_end = flows_data.shape[0]\n",
    "y_raw = y_data[i_start:i_end,:]\n",
    "imu_data = imu_data[i_start:i_end,:]\n",
    "flows_data = flows_data[i_start:i_end,:,:]\n",
    "\n",
    "pos_err = pos_err[i_start:i_end]\n",
    "pos_err_rate = pos_err_rate[i_start:i_end]\n",
    "\n",
    "# Get sign from cumulative error:\n",
    "y_cumulative = np.cumsum(y_data,axis=0)\n",
    "\n",
    "# Convert y_data into classes:\n",
    "y_norm = np.zeros(y_raw.shape[0]);\n",
    "y_norm_c = np.zeros(y_raw.shape[0]+1);\n",
    "for k in range(y_raw.shape[0]):\n",
    "    y_norm[k]=np.linalg.norm(y_raw[k,:])\n",
    "    y_norm_c[k+1] = np.linalg.norm(y_cumulative[k,:])\n",
    "\n",
    "    \n",
    "\n",
    "y_norm = pos_err_rate\n",
    "\n",
    "minval = np.min(y_norm)\n",
    "maxval = np.max(y_norm)\n",
    "\n",
    "\n",
    "NUM_CLASSES = 128\n",
    "import utilities\n",
    "y_data, mean_vals = utilities.discretize_outputs(y_norm, NUM_CLASSES)\n",
    "\n",
    "\n",
    "# Normalize imu data:\n",
    "imu_data = imu_data - imu_data.mean(axis=0)\n",
    "imu_data /= np.std(imu_data,axis=0)\n",
    "\n",
    "# Normalize image data\n",
    "flows_data -= flows_data.mean(axis=0)\n",
    "flows_data /= np.std(flows_data,axis=0)\n",
    "\n",
    "print('Organizing train/val split...')\n",
    "N_training = int(0.6*flows_data.shape[0])\n",
    "N_validation = flows_data.shape[0] - N_training\n",
    "#training_data_indices = np.random.choice(np.arange(flows_data.shape[0]), N_training)\n",
    "#validation_data_indices = np.setdiff1d(np.arange(flows_data.shape[0]), training_data_indices)\n",
    "\n",
    "training_data_indices = np.arange(0,N_training,1,dtype=np.int32)\n",
    "validation_data_indices = N_training+np.arange(0, flows_data.shape[0]-N_training,1,dtype=np.int32)\n",
    "tmp, mean_vals = utilities.discretize_outputs(y_norm[training_data_indices], NUM_CLASSES)\n",
    "\n",
    "print(training_data_indices.shape)\n",
    "print(N_validation)\n",
    "\n",
    "# sample batch_size subsequences of length sequence_length, and return the label at the end of the sequence\n",
    "# These need to be randomized more - getting unwanted structure in the train/val split\n",
    "def sample_minibatch(batch_size, train=True,iseq=None):\n",
    "    x_img_batch = np.zeros((batch_size, flows_data.shape[1], flows_data.shape[2],1))\n",
    "    y_batch = np.zeros(batch_size)\n",
    "    \n",
    "    if(iseq is None):\n",
    "        # randomly sample endpoint:\n",
    "        # choose which subsequence to sample from: \n",
    "        if(train):\n",
    "            i_vals = training_data_indices[np.random.randint(0,N_training,batch_size)]\n",
    "        else:\n",
    "            i_vals = validation_data_indices[np.random.randint(0,N_validation,batch_size)]\n",
    "            i_stop = 0\n",
    "    else:\n",
    "        i_vals = i_seq\n",
    "    y_batch = y_data[i_vals]\n",
    "    x_img_batch = flows_data[i_vals,:,:].reshape((batch_size, flows_data.shape[1], flows_data.shape[2],1))\n",
    "    return x_img_batch, y_batch\n",
    "\n",
    "def calc_rmse(predictions, targets):\n",
    "    \n",
    "    return np.sqrt(((predictions.reshape([-1]) - targets.reshape([-1])) ** 2).mean())\n",
    "if False:\n",
    "    print('Visualizing data...')\n",
    "    print('Training vs. Validation sets:')\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(imu_data)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(y_norm)\n",
    "    #p = plt.axhspan(0.25, 0.75, facecolor='0.5', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(y_norm_c)\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(y_data[training_data_indices])\n",
    "    plt.title('Histogram of classes in training')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(y_data[validation_data_indices])\n",
    "    plt.title('Histogram of classes in validation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def fire_module(x,inp,sp,e11p,e33p):\n",
    "    with tf.variable_scope(\"fire\"):\n",
    "        with tf.variable_scope(\"squeeze\"):\n",
    "            W = tf.get_variable(\"weights\",shape=[1,1,inp,sp])\n",
    "            b = tf.get_variable(\"bias\",shape=[sp])\n",
    "            s = tf.nn.conv2d(x,W,[1,1,1,1],\"VALID\")+b\n",
    "            s = tf.nn.relu(s)\n",
    "        with tf.variable_scope(\"e11\"):\n",
    "            W = tf.get_variable(\"weights\",shape=[1,1,sp,e11p])\n",
    "            b = tf.get_variable(\"bias\",shape=[e11p])\n",
    "            e11 = tf.nn.conv2d(s,W,[1,1,1,1],\"VALID\")+b\n",
    "            e11 = tf.nn.relu(e11)\n",
    "        with tf.variable_scope(\"e33\"):\n",
    "            W = tf.get_variable(\"weights\",shape=[3,3,sp,e33p])\n",
    "            b = tf.get_variable(\"bias\",shape=[e33p])\n",
    "            e33 = tf.nn.conv2d(s,W,[1,1,1,1],\"SAME\")+b\n",
    "            e33 = tf.nn.relu(e33)\n",
    "        return tf.concat([e11,e33],3)\n",
    "\n",
    "    # input is a factor of 3 skinnier and 4 shorter than expected. So pool less by a factor of 3 and 4?\n",
    "    # 13x13\n",
    "    # 4x3\n",
    "\n",
    "class SqueezeNet(object):\n",
    "    def extract_features(self, input=None, reuse=True):\n",
    "        if input is None:\n",
    "            input = self.image\n",
    "        x = input\n",
    "        layers = []\n",
    "        with tf.variable_scope('features', reuse=reuse):\n",
    "            with tf.variable_scope('layer0'):\n",
    "                W = tf.get_variable(\"weights\",shape=[3,3,1,64])\n",
    "                b = tf.get_variable(\"bias\",shape=[64])\n",
    "                x = tf.nn.conv2d(x,W,[1,2,2,1],\"VALID\")\n",
    "                x = tf.nn.bias_add(x,b)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer1'):\n",
    "                x = tf.nn.relu(x)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer2'):\n",
    "                x = tf.nn.max_pool(x,[1,3,3,1],strides=[1,2,2,1],padding='VALID')\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer3'):\n",
    "                x = fire_module(x,64,16,64,64)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer4'):\n",
    "                x = fire_module(x,128,16,64,64)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer5'):\n",
    "                x = tf.nn.max_pool(x,[1,3,3,1],strides=[1,2,2,1],padding='VALID')\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer6'):\n",
    "                x = fire_module(x,128,32,128,128)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer7'):\n",
    "                x = fire_module(x,256,32,128,128)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer8'):\n",
    "                x = tf.nn.max_pool(x,[1,3,3,1],strides=[1,2,2,1],padding='VALID')\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer9'):\n",
    "                x = fire_module(x,256,48,192,192)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer10'):\n",
    "                x = fire_module(x,384,48,192,192)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer11'):\n",
    "                x = fire_module(x,384,64,256,256)\n",
    "                layers.append(x)\n",
    "            with tf.variable_scope('layer12'):\n",
    "                x = fire_module(x,512,64,256,256)\n",
    "                layers.append(x)\n",
    "        return layers\n",
    "\n",
    "    def __init__(self, save_path=None, sess=None):\n",
    "        \"\"\"Create a SqueezeNet model.\n",
    "        Inputs:\n",
    "        - save_path: path to TensorFlow checkpoint\n",
    "        - sess: TensorFlow session\n",
    "        \"\"\"\n",
    "        self.image = tf.placeholder('float',shape=[None,None,None,1],name='input_image')\n",
    "        self.labels = tf.placeholder('int32', shape=[None], name='labels')\n",
    "        self.lr = tf.placeholder('float',shape=[1],name='lr')\n",
    "        self.layers = []\n",
    "        x = self.image\n",
    "        self.layers = self.extract_features(x, reuse=False)\n",
    "        self.features = self.layers[-1]\n",
    "        with tf.variable_scope('classifier'):\n",
    "            with tf.variable_scope('layer0'):\n",
    "                x = self.features\n",
    "                self.layers.append(x)\n",
    "            with tf.variable_scope('layer1'):\n",
    "                W = tf.get_variable(\"weights\",shape=[1,1,512,1000])\n",
    "                b = tf.get_variable(\"bias\",shape=[1000])\n",
    "                x = tf.nn.conv2d(x,W,[1,1,1,1],\"VALID\")\n",
    "                x = tf.nn.bias_add(x,b)\n",
    "                self.layers.append(x)\n",
    "            with tf.variable_scope('layer2'):\n",
    "                x = tf.nn.relu(x)\n",
    "                self.layers.append(x)\n",
    "            with tf.variable_scope('layer3'):\n",
    "                x = tf.nn.avg_pool(x,[1,6,5,1],strides=[1,4,4,1],padding='VALID')\n",
    "                self.layers.append(x)\n",
    "# NOT really squeezenet, but ...\n",
    "            with tf.variable_scope('layer4'):\n",
    "                x = tf.layers.dense(x, NUM_CLASSES)\n",
    "                self.layers.append(x)\n",
    "        print(x.get_shape())\n",
    "        self.classifier = tf.reshape(x,[BATCH_SIZE, NUM_CLASSES])\n",
    "\n",
    "        if save_path is not None:\n",
    "            saver = tf.train.Saver()\n",
    "#            saver = tf.train.import_meta_graph(save_path*'.meta')\n",
    "            saver.restore(sess, save_path)\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(self.labels, NUM_CLASSES), logits=self.classifier))\n",
    "        self.prediction = tf.cast(tf.argmax(self.classifier,1),'int32')\n",
    "        self.acc = tf.reduce_mean(tf.cast(tf.equal(self.prediction, self.labels),tf.float32))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr[0]).minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Blue background corresponds to data used during training\n",
    "def plot_data(ns, ne, save=False):    \n",
    "\n",
    "    print('plotting from ', ns, ' to ', ne)\n",
    "    \n",
    "    print('High correlation (near 1) means we are predicting well.')\n",
    "    m1 = y_norm[ns:ne].mean()\n",
    "    c1 = np.sqrt(np.correlate(y_norm[ns:ne]-m1,y_norm[ns:ne]-m1)[0])\n",
    "    m2 = r[ns:ne].mean()\n",
    "    c2 = np.sqrt(np.correlate(r[ns:ne]-m2,r[ns:ne]-m2)[0])\n",
    "    c12 = (np.correlate(y_norm[ns:ne]-m1,r[ns:ne]-m2))[0]/(c1*c2)\n",
    "    print('Correlation between error and signal:', (np.abs(c12)))\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.ylabel('Error Rate')\n",
    "#    plt.axvspan(0,0.6*y_norm.shape[0],facecolor='b',alpha=0.1)\n",
    "#    plt.axvspan(0.6*y_norm.shape[0],y_norm.shape[0],facecolor='g',alpha=0.1)\n",
    "    plt.plot(y_norm[ns:ne],'.',alpha=0.6)\n",
    "    plt.plot(r[ns:ne],'-.',alpha=0.6)\n",
    "#    plt.xlim([ns,ne])\n",
    "    plt.legend(['Actual', 'Predicted'])\n",
    "    plt.plot([0,ne-ns],[0,0],':')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.axvspan(0,0.6*y_norm.shape[0],facecolor='b',alpha=0.1)\n",
    "    plt.axvspan(0.6*y_norm.shape[0],y_norm.shape[0],facecolor='g',alpha=0.1)\n",
    "    plt.ylabel('Cumulative Error')\n",
    "    plt.plot(np.cumsum(r[ns:ne]))\n",
    "    plt.plot(np.cumsum(y_norm[ns:ne]))\n",
    "    plt.plot([0,ne-ns],[0,0],':')\n",
    "    plt.legend(['Predicted','Actual'])\n",
    "    \n",
    "    if(save):\n",
    "        plt.savefig('pred.png',dpi=720)\n",
    "    else:\n",
    "        plt.show()\n",
    "    return plt\n",
    "\n",
    "def plot_error(ns, ne):\n",
    "    print('High correlation (near 1) means we are predicting noise.')\n",
    "    m1 = y_norm[ns:ne].mean()\n",
    "    c1 = np.sqrt(np.correlate(y_norm[ns:ne]-m1,y_norm[ns:ne]-m1)[0])\n",
    "    m2 = np.mean(r[ns:ne]-y_norm[ns:ne])\n",
    "    c2 = np.sqrt(np.correlate(r[ns:ne]-m2-y_norm[ns:ne],r[ns:ne]-m2-y_norm[ns:ne])[0])\n",
    "    c12 = (np.correlate(y_norm[ns:ne]-m1,r[ns:ne]-y_norm[ns:ne]-m2))[0]/(c1*c2)\n",
    "    print('Correlation between error and signal:', (np.abs(c12)))\n",
    "    \n",
    "    plt.ylabel('Prediction Errors')\n",
    "    plt.plot(r[ns:ne]-y_norm[ns:ne])\n",
    "    plt.plot(-y_norm[ns:ne],':')\n",
    "    plt.legend(['Error', 'Signal'])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # remove all existing variables in the graph \n",
    "sess = get_session() # start a new Session\n",
    "\n",
    "# simple parameters:\n",
    "disp_period = 100\n",
    "training_iters = 24000\n",
    "lr = 0.0002 # 0.0002 is a good value for 20 bins\n",
    "lr_decay=0.99\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    tf.reset_default_graph()\n",
    "    sess = get_session()\n",
    "    model = SqueezeNet(sess=sess)\n",
    "\n",
    "        # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    train_inds = []\n",
    "\n",
    "    naive_val = 1/NUM_CLASSES\n",
    "    \n",
    "    # Launch the graph\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    loss_hist = []\n",
    "    train_inds = []\n",
    "    val_inds = []\n",
    "    smoothed_train = []\n",
    "    smooth_val = 0.999\n",
    "    # Keep training until reach max iterations\n",
    "    while step  < training_iters:\n",
    "        if step % 30 is 0:\n",
    "            lr *= lr_decay\n",
    "        # these batches are of size 1?\n",
    "        # for now, just grab a random sequence of data:\n",
    "        batch_x, batch_y = sample_minibatch(BATCH_SIZE)        \n",
    "        \n",
    "#        print('Batch_y shape:', batch_y.shape)\n",
    "        # Run optimization op (backprop)\n",
    "#                self.image = tf.placeholder('float',shape=[None,None,None,3],name='input_image')\n",
    "#        self.labels = tf.placeholder('int32', shape=[None], name='labels')\n",
    "#        self.lr = tf.placeholder('float',shape[1],name='lr')\n",
    "\n",
    "        o,loss,acc_t = sess.run([model.optimizer,model.loss, model.acc], feed_dict={model.image: batch_x, model.labels: batch_y, model.lr:[lr]})\n",
    "        # Compute accuracy:\n",
    "        train_inds.append(step)\n",
    "        train_history.append(acc_t)\n",
    "        loss_hist.append(loss)\n",
    "        if(step is 1):\n",
    "            smoothed_train.append(loss)\n",
    "        else:\n",
    "            smoothed_train.append(smooth_val*smoothed_train[-1] + (1-smooth_val)*loss)\n",
    "        \n",
    "        step += 1\n",
    "        if step % disp_period is 0:\n",
    "            # Calculate val accuracy:\n",
    "            batch_x, batch_y = sample_minibatch(BATCH_SIZE, False)\n",
    "            acc_v = sess.run(model.acc, feed_dict={model.image: batch_x, model.labels: batch_y})\n",
    "            val_history.append(acc_v)\n",
    "            val_inds.append(step)\n",
    "            \n",
    "            print('Step: ', step, ' Train: ', acc_t, ' Val: ', acc_v, 'loss',loss_hist[-1], 'LR: ', lr)\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(train_inds, train_history,alpha=0.9)\n",
    "            plt.plot(val_inds, val_history,alpha=0.9)\n",
    "            plt.plot([0,step], [naive_val,naive_val])\n",
    "            plt.legend(['Training','Validation','Naive'])\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Acc.')\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.plot(train_inds,loss_hist)\n",
    "            plt.plot(train_inds, smoothed_train, linewidth=4)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.show()\n",
    "            \n",
    "            # Prediction is cheap for this:\n",
    "            y_pred = np.zeros_like(y_data)\n",
    "\n",
    "            for k in range(0,flows_data.shape[0]-(flows_data.shape[0]%BATCH_SIZE),BATCH_SIZE):\n",
    "                imgs = flows_data[k:k+BATCH_SIZE,:,:].reshape(BATCH_SIZE,flows_data.shape[1],flows_data.shape[2],1)\n",
    "                pred = sess.run(model.prediction, feed_dict={model.image: imgs, model.labels: y_data[k:k+BATCH_SIZE]})\n",
    "                y_pred[k:k+BATCH_SIZE] = pred\n",
    "            r = mean_vals[(y_pred[:]).astype(np.int32)];\n",
    "\n",
    "            plot_data(0,y_data.shape[0])\n",
    "\n",
    "            \n",
    "    # Compute sequence prediction:\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstruct signal using a randomwalk model:\n",
    "print('Computing sequence prediction - this may take a while. ')\n",
    "y_pred = np.zeros_like(y_data)\n",
    "\n",
    "for k in range(0,flows_data.shape[0]-(flows_data.shape[0]%BATCH_SIZE),BATCH_SIZE):\n",
    "    if k % 1000 is 0:\n",
    "        print('k = ', k)\n",
    "    pred = sess.run(model.prediction, feed_dict={model.image: flows_data[k:k+BATCH_SIZE,:,:,:], model.labels: y_data[k:k+BATCH_SIZE]})\n",
    "    y_pred[k:k+BATCH_SIZE] = pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can load a previously computed prediction - useful if cold starting a notebook.\n",
    "r = mean_vals[(y_pred[:]).astype(np.int32)];\n",
    "np.save('cnn_predictions',r)\n",
    "\n",
    "y_norm = pos_err_rate\n",
    "\n",
    "print('RMSE: ',utilities.calc_rmse(r, y_norm))\n",
    "\n",
    "# smooth the reconstructed signal:\n",
    "#r = np.zeros_like(reconstructed_signal)\n",
    "#alpha = 1.0\n",
    "#for k in range(reconstructed_signal.shape[0]):\n",
    "#    if(k==0):\n",
    "#        r[k] = reconstructed_signal[0]\n",
    "#    else:\n",
    "#        r[k] = alpha*reconstructed_signal[k] + (1-alpha)*r[k-1]\n",
    "        \n",
    "plt.plot(y_norm)\n",
    "plt.plot(training_data_indices, r[training_data_indices],'.')\n",
    "plt.plot(validation_data_indices, r[validation_data_indices],'.')\n",
    "plt.legend(['Actual','Train','Val'])\n",
    "plt.show()\n",
    "\n",
    "#plt.title('Learning Curve')\n",
    "#naive_val = 0\n",
    "#for k in range(y_data.shape[0]):\n",
    "#    naive_val += calc_rmse(y_data[k,:],np.zeros(OUTPUT_DIM))\n",
    "#naive_val /= y_data.shape[0]\n",
    "\n",
    "#plt.plot(train_history,'.',alpha=0.3)\n",
    "#plt.plot(val_history,'.',alpha=0.3)\n",
    "#plt.plot(naive_val*np.ones_like(train_history),':')\n",
    "#plt.legend(['Training','Validation','Naive'])\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('RMSE')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "plot_data(0,y_data.shape[0])\n",
    "plot_error(300,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_vals[(y_pred[:]).astype(np.int32)])\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "    n_disc\n",
    "    true_disc, mv = utilities.discretize_outputs(y_data, n_disc)\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,1,1);\n",
    "    plt.plot(pos_err)\n",
    "    plt.plot()\n",
    "    plt.subplot(2,1,2)\n",
    "#    plt.plot(y_data)\n",
    "#    plt.plot(mean_vals[true_disc.astype(np.int32)],'o',alpha=0.3)\n",
    "    plt.plot(mean_vals[y_pred.astype(np.int32)],'x',alpha=0.9)\n",
    "#    plt.ylim([-0.02,0.02])\n",
    "    plt.xlim([60000,70000])\n",
    "    plt.show()\n",
    "    plt.plot(d_raw[:,4],'.')\n",
    "    plt.xlim([60000,70000])\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "    plt.plot(mean_vals)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
