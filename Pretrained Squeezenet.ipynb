{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def get_session():\n",
    "    \"\"\"Create a session that dynamically allocates memory.\"\"\"\n",
    "    # See: https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utilities\n",
    "\n",
    "### Global variables ###\n",
    "TRAIN_RATIO = 0.6    # Fraction of data to use for training\n",
    "#VAL_RATIO   = 0.2    # Fraction of data to use for validation\n",
    "N_LEVELS    = 20\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector data\n",
      "Loading image data\n",
      "Preprocessing data\n",
      "float32\n",
      "(5640, 192, 192)\n",
      "(64, 192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from csv files:\n",
    "tmp, x_data, y_norm = utilities.load_data()\n",
    "print(x_data.dtype)\n",
    "\n",
    "# Preprocess for squeezenet\n",
    "\n",
    "\n",
    "N_training = int(TRAIN_RATIO*x_data.shape[0])\n",
    "N_validation = x_data.shape[0]- N_training\n",
    "\n",
    "y_data, mean_vals = utilities.discretize_outputs(y_norm[0:N_training], N_LEVELS)\n",
    "y_data, tmp = utilities.discretize_outputs(y_norm, N_LEVELS)\n",
    "\n",
    "x_train = x_data[0:N_training,...]\n",
    "y_train = y_data[0:N_training]\n",
    "\n",
    "x_val = x_data[N_training:,...]\n",
    "y_val = y_data[N_training:]\n",
    "\n",
    "print(x_train.shape)\n",
    "batch_x, batch_y = utilities.sample_3x(BATCH_SIZE, x_train, y_train)\n",
    "print(batch_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers.squeezenet import SqueezeNet\n",
    "\n",
    "tf.reset_default_graph() # remove all existing variables in the graph \n",
    "sess = get_session() # start a new Session\n",
    "# Load pretrained SqueezeNet model\n",
    "SAVE_PATH = 'classifiers/squeezenet.ckpt'\n",
    "#if not os.path.exists(SAVE_PATH):\n",
    "#    raise ValueError(\"You need to download SqueezeNet!\")\n",
    "\n",
    "model = SqueezeNet(N_LEVELS, sess=sess)\n",
    "#model = SqueezeNet(N_LEVELS,save_path=SAVE_PATH, sess=sess)\n",
    "\n",
    "\n",
    "uninitialized_vars = []\n",
    "for var in tf.global_variables():\n",
    "    try:\n",
    "        sess.run(var)\n",
    "    except tf.errors.FailedPreconditionError:\n",
    "        uninitialized_vars.append(var)\n",
    "        \n",
    "init = tf.variables_initializer(uninitialized_vars)\n",
    "sess.run(init)\n",
    "#print(uninitialized_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now train!\n",
    "\n",
    "disp_period = 1\n",
    "save_period = 1000\n",
    "training_iters = 40000\n",
    "\n",
    "\n",
    "print('initializing...')\n",
    "lr = 5e-3\n",
    "lr_decay = 0.99\n",
    "with tf.device('/cpu:0'):\n",
    "        # Initializing the variables\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    naive_history = []\n",
    "    \n",
    "    # Launch the graph\n",
    "    step = 1\n",
    "    ind_train = []\n",
    "    train_inds = []\n",
    "    val_inds = []\n",
    "\n",
    "    smoothed_train = []\n",
    "    smooth_val = 1-0.999\n",
    "\n",
    "    print('Training')\n",
    "    # Keep training until reach max iterations\n",
    "    while step  < training_iters:\n",
    "        if step % 100 is 0:\n",
    "            lr *= lr_decay\n",
    "        if step % 2000 is 0:\n",
    "            lr *= 0.1\n",
    "        lr = np.max([lr, 0.00000001])\n",
    "        # these batches are of size 1?\n",
    "        # for now, just grab a random sequence of data:\n",
    "        batch_x, batch_y = utilities.sample_3x(BATCH_SIZE, x_train, y_train)\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        o,mse = sess.run([model.optimizer,model.loss], feed_dict={model.image: batch_x, model.labels: batch_y, model.lr:[lr]})\n",
    "        train_inds.append(step)\n",
    "        if step > 1:\n",
    "            train_history.append(mse*smooth_val + (1-smooth_val)*train_history[-1])\n",
    "        else:\n",
    "            train_history.append(mse)\n",
    "            \n",
    "        step += 1\n",
    "        if step % disp_period is 0:\n",
    "            # Calculate val accuracy:\n",
    "            N_smooth = 10;\n",
    "            batch_x, batch_y = utilities.sample_3x(BATCH_SIZE, x_val, y_val)\n",
    "            vmse = sess.run(model.loss, feed_dict={model.image: batch_x, model.labels: batch_y})\n",
    "            val_history.append(vmse/N_smooth)\n",
    "            val_inds.append(step)\n",
    "            \n",
    "            for smooth_num in range(N_smooth-1):\n",
    "                batch_x, batch_y = utilities.sample_3x(BATCH_SIZE, x_val, y_val)\n",
    "                vmse = sess.run(model.loss, feed_dict={model.image: batch_x, model.labels: batch_y})\n",
    "                val_history[-1]+=(vmse/N_smooth)\n",
    "\n",
    "            \n",
    "            print('Step: ', step, ' MSE: ', train_history[-1], ' Val: ', val_history[-1], 'LR: ', lr)\n",
    "            plt.semilogy(train_inds, train_history,alpha=0.9)\n",
    "            plt.semilogy(val_inds, val_history,alpha=0.6)\n",
    "            plt.semilogy([0,step],[0.0069,0.0069],':')\n",
    "            plt.legend(['Training','Validation','Constant est.'])\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.savefig('pretrained_net_perf.png')\n",
    "            \n",
    "        if step % save_period is 0:\n",
    "            print('Computing sequence prediction - this may take a while. ')\n",
    "            y_pred = np.zeros_like(y_data)\n",
    "\n",
    "            for k in range(0,x_data.shape[0]-(x_data.shape[0]%BATCH_SIZE),BATCH_SIZE):\n",
    "                if k % 1024 is 0:\n",
    "                    print('k = ', k)\n",
    "                bx,by = utilities.sample_3x(BATCH_SIZE,x_data,y_data,k)\n",
    "                pred = sess.run(model.prediction, feed_dict={model.image: bx})\n",
    "                y_pred[k:k+BATCH_SIZE] = pred\n",
    "            np.save('pretrained_cnn_pred',y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstruct signal using a randomwalk model:\n",
    "print('Computing sequence prediction - this may take a while. ')\n",
    "y_pred = np.zeros_like(y_data)\n",
    "\n",
    "for k in range(0,x_data.shape[0]-(x_data.shape[0]%BATCH_SIZE),BATCH_SIZE):\n",
    "    if k % 1024 is 0:\n",
    "        print('k = ', k)\n",
    "    bx,by = utilities.sample_3x(BATCH_SIZE,x_data,y_data,k)\n",
    "    pred = sess.run(model.prediction, feed_dict={model.image: bx})\n",
    "    y_pred[k:k+BATCH_SIZE] = pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Blue background corresponds to data used during training\n",
    "def plot_data(ns, ne, save=False):    \n",
    "\n",
    "    print('plotting from ', ns, ' to ', ne)\n",
    "    \n",
    "    print('High correlation (near 1) means we are predicting well.')\n",
    "    m1 = y_norm[ns:ne].mean()\n",
    "    c1 = np.sqrt(np.correlate(y_norm[ns:ne]-m1,y_norm[ns:ne]-m1)[0])\n",
    "    m2 = r[ns:ne].mean()\n",
    "    c2 = np.sqrt(np.correlate(r[ns:ne]-m2,r[ns:ne]-m2)[0])\n",
    "    c12 = (np.correlate(y_norm[ns:ne]-m1,r[ns:ne]-m2))[0]/(c1*c2)\n",
    "    print('Correlation between error and signal:', (np.abs(c12)))\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.ylabel('Error Rate')\n",
    "#    plt.axvspan(0,0.6*y_norm.shape[0],facecolor='b',alpha=0.1)\n",
    "#    plt.axvspan(0.6*y_norm.shape[0],y_norm.shape[0],facecolor='g',alpha=0.1)\n",
    "    plt.plot(y_norm[ns:ne],'.',alpha=0.6)\n",
    "    plt.plot(r[ns:ne],'-.',alpha=0.6)\n",
    "#    plt.xlim([ns,ne])\n",
    "    plt.legend(['Actual', 'Predicted'])\n",
    "    plt.plot([0,ne-ns],[0,0],':')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.axvspan(0,0.6*y_norm.shape[0],facecolor='b',alpha=0.1)\n",
    "    plt.axvspan(0.6*y_norm.shape[0],y_norm.shape[0],facecolor='g',alpha=0.1)\n",
    "    plt.ylabel('Cumulative Error')\n",
    "    plt.plot(np.cumsum(r[ns:ne]))\n",
    "    plt.plot(np.cumsum(y_norm[ns:ne]))\n",
    "    plt.plot([0,ne-ns],[0,0],':')\n",
    "    plt.legend(['Predicted','Actual'])\n",
    "    \n",
    "    if(save):\n",
    "        plt.savefig('pred.png',dpi=720)\n",
    "    else:\n",
    "        plt.show()\n",
    "    return plt\n",
    "\n",
    "def plot_error(ns, ne):\n",
    "    print('High correlation (near 1) means we are predicting noise.')\n",
    "    m1 = y_norm[ns:ne].mean()\n",
    "    c1 = np.sqrt(np.correlate(y_norm[ns:ne]-m1,y_norm[ns:ne]-m1)[0])\n",
    "    m2 = np.mean(r[ns:ne]-y_norm[ns:ne])\n",
    "    c2 = np.sqrt(np.correlate(r[ns:ne]-m2-y_norm[ns:ne],r[ns:ne]-m2-y_norm[ns:ne])[0])\n",
    "    c12 = (np.correlate(y_norm[ns:ne]-m1,r[ns:ne]-y_norm[ns:ne]-m2))[0]/(c1*c2)\n",
    "    print('Correlation between error and signal:', (np.abs(c12)))\n",
    "    \n",
    "    plt.ylabel('Prediction Errors')\n",
    "    plt.plot(r[ns:ne]-y_norm[ns:ne])\n",
    "    plt.plot(-y_norm[ns:ne],':')\n",
    "    plt.legend(['Error', 'Signal'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Can load a previously computed prediction - useful if cold starting a notebook.\n",
    "r = mean_vals[(y_pred[:]).astype(np.int32)];\n",
    "np.save('scnn_predictions',r)\n",
    "\n",
    "print('RMSE: ',utilities.calc_rmse(r, y_norm))\n",
    "\n",
    "# smooth the reconstructed signal:\n",
    "#r = np.zeros_like(reconstructed_signal)\n",
    "#alpha = 1.0\n",
    "#for k in range(reconstructed_signal.shape[0]):\n",
    "#    if(k==0):\n",
    "#        r[k] = reconstructed_signal[0]\n",
    "#    else:\n",
    "#        r[k] = alpha*reconstructed_signal[k] + (1-alpha)*r[k-1]\n",
    "        \n",
    "plt.plot(y_norm)\n",
    "plt.plot(training_data_indices, r[training_data_indices],'.')\n",
    "plt.plot(validation_data_indices, r[validation_data_indices],'.')\n",
    "plt.legend(['Actual','Train','Val'])\n",
    "plt.show()\n",
    "\n",
    "#plt.title('Learning Curve')\n",
    "#naive_val = 0\n",
    "#for k in range(y_data.shape[0]):\n",
    "#    naive_val += calc_rmse(y_data[k,:],np.zeros(OUTPUT_DIM))\n",
    "#naive_val /= y_data.shape[0]\n",
    "\n",
    "#plt.plot(train_history,'.',alpha=0.3)\n",
    "#plt.plot(val_history,'.',alpha=0.3)\n",
    "#plt.plot(naive_val*np.ones_like(train_history),':')\n",
    "#plt.legend(['Training','Validation','Naive'])\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('RMSE')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "plot_data(0,y_data.shape[0])\n",
    "plot_error(300,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute discretization and check its error:\n",
    "y_d,binvals = utilities.discretize_outputs(err_norm[train_inds],N_LEVELS)\n",
    "y_d,bv = utilities.discretize_outputs(err_norm,N_LEVELS)\n",
    "\n",
    "\n",
    "print('RMSE from discretziation', utilities.calc_rmse(binvals[(y_d[:]).astype(np.int32)], err_norm))\n",
    "print('RMSE from naive mean guess', utilities.calc_rmse(err_norm, np.mean(err_norm)*np.ones_like(err_norm)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(err_norm)\n",
    "plt.plot(binvals[(y_d[:]).astype(np.int32)],alpha=0.3);\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(binvals,'.')\n",
    "plt.plot(bv,'.')\n",
    "\n",
    "# Discretized value:\n",
    "y_norm_disc = binvals[(y_d[:]).astype(np.int32)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Play around with the SQSS and see if we can get it to work:\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "N_steps = 10\n",
    "\n",
    "#data_dict['imu'] = tf.placeholder('float',shape=[N_steps,None,None,2],name='input_image')\n",
    "\n",
    "data_dict['x_acc'] = imu_data[:,0]\n",
    "data_dict['y_acc'] = imu_data[:,1]\n",
    "data_dict['z_acc'] = imu_data[:,2]\n",
    "data_dict['x_gyr'] = imu_data[:,3]\n",
    "data_dict['y_gyr'] = imu_data[:,4]\n",
    "data_dict['z_gyr'] = imu_data[:,5]\n",
    "data_dict['flows'] = imu_data[:,6]\n",
    "data_keys = data_dict.keys()\n",
    "\n",
    "data_cntxt['x_acc'] = (imu_data[:,0].mean()\n",
    "data_cntxt['y_acc'] = (imu_data[:,1].mean()\n",
    "data_cntxt['z_acc'] = (imu_data[:,2].mean()\n",
    "data_cntxt['x_gyr'] = (imu_data[:,3].mean()\n",
    "data_cntxt['y_gyr'] = (imu_data[:,4].mean()\n",
    "data_cntxt['z_gyr'] = (imu_data[:,5].mean()\n",
    "data_cntxt['flows'] = (imu_data[:,6]).mean()\n",
    "\n",
    "batch_size = 32\n",
    "num_unroll = 20\n",
    "num_enqueue_threads = 4\n",
    "lstm_size = 64\n",
    "state_size = 64\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size)\n",
    "\n",
    "initial_state_values = tf.zeros((state_size,), dtype=tf.float32)\n",
    "initial_states = {\"lstm_state\": initial_state_values}\n",
    "\n",
    "batch = tf.batch_sequences_with_states(\n",
    "    input_key=data_dict.keys(),\n",
    "    input_sequences=data_dict,\n",
    "    input_context=data_cntxt,\n",
    "    input_length=tf.shape(data[\"x_acc\"])[0],\n",
    "    initial_states=initial_states,\n",
    "    num_unroll=num_unroll,\n",
    "    batch_size=batch_size,\n",
    "    num_threads=num_enqueue_threads,\n",
    "    capacity=batch_size * num_enqueue_threads * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
